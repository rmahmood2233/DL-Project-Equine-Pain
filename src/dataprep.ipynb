{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235d8b42",
   "metadata": {},
   "source": [
    "#### --- Import Libraries ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9cc8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbf1f6",
   "metadata": {},
   "source": [
    "#### --- Config & Setup ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_root = os.getcwd()  # Gets your current working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e7bf5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All folders and paths set up correctly.\n",
      "S2_PATH: D:\\5TH SEM\\DL\\DL PROJECT\\DL_Project_Equine Pain\\EQUINE PAIN CODE\\dataset\\annotations\\pone.0231608.s002.xlsx\n",
      "S3_PATH: D:\\5TH SEM\\DL\\DL PROJECT\\DL_Project_Equine Pain\\EQUINE PAIN CODE\\dataset\\annotations\\pone.0231608.s003.xlsx\n",
      "VIDEOS_DIR: D:\\5TH SEM\\DL\\DL PROJECT\\DL_Project_Equine Pain\\EQUINE PAIN CODE\\dataset\\videos\n",
      "CLIPS_DIR: D:\\5TH SEM\\DL\\DL PROJECT\\DL_Project_Equine Pain\\EQUINE PAIN CODE\\outputs\\clips\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "S2_PATH = os.path.join(project_root, 'dataset', 'annotations', 'pone.0231608.s002.xlsx')\n",
    "S3_PATH = os.path.join(project_root, 'dataset', 'annotations', 'pone.0231608.s003.xlsx')\n",
    "VIDEOS_DIR = os.path.join(project_root, 'dataset', 'videos')\n",
    "CLIPS_DIR = os.path.join(project_root, 'outputs', 'clips')\n",
    "\n",
    "# Create all needed subfolders (eyes/chin/negative/csv) robustly\n",
    "folders = [\n",
    "    os.path.join(CLIPS_DIR, 'eyes_positive'),\n",
    "    os.path.join(CLIPS_DIR, 'chin_positive'),\n",
    "    os.path.join(CLIPS_DIR, 'negative'),\n",
    "    os.path.join(project_root, 'outputs', 'csv')\n",
    "]\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"All folders and paths set up correctly.\")\n",
    "print(\"S2_PATH:\", S2_PATH)\n",
    "print(\"S3_PATH:\", S3_PATH)\n",
    "print(\"VIDEOS_DIR:\", VIDEOS_DIR)\n",
    "print(\"CLIPS_DIR:\", CLIPS_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5e5a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 rows of raw S3 annotation:\n",
      "                          0        1   2             3   4             5   \\\n",
      "0                        NaN  FILM 1  NaN           NaN NaN           NaN   \n",
      "1   Start/End time: hh:mm:ss     Code NaN  Duration (s) NaN   Start time    \n",
      "2                        NaN    VC71R NaN         28.63 NaN  00:00:00.000   \n",
      "3                        NaN    VC72R NaN         28,63 NaN  00:00:00.000   \n",
      "4                        NaN     AD51 NaN          1,82 NaN  00:00:00.100   \n",
      "5                        NaN    AU47L NaN          0,23 NaN  00:00:00.220   \n",
      "6                        NaN    AU101 NaN          1,48 NaN  00:00:00.470   \n",
      "7                        NaN     AD51 NaN          0,18 NaN  00:00:01.874   \n",
      "8                        NaN    AU47L NaN          0,37 NaN  00:00:01.930   \n",
      "9                        NaN     AD1L NaN          0,71 NaN  00:00:02.290   \n",
      "10                       NaN  EAD104R NaN          0,76 NaN  00:00:02.300   \n",
      "11                       NaN   AU101L NaN          1,96 NaN  00:00:02.320   \n",
      "12                       NaN     AD51 NaN          1,92 NaN  00:00:03.920   \n",
      "13                       NaN     AU5L NaN         12,29 NaN  00:00:04.370   \n",
      "14                       NaN  EAD101R NaN          0,82 NaN  00:00:07.420   \n",
      "\n",
      "              6   7        8   9   ...  73            74            75  76  \\\n",
      "0            NaN NaN   FILM 2 NaN  ... NaN           NaN           NaN NaN   \n",
      "1      End time  NaN     Code NaN  ... NaN   Start time       End time NaN   \n",
      "2   00:00:28.633 NaN    VC70R NaN  ... NaN  00:00:00.010  00:00:00.990 NaN   \n",
      "3   00:00:28.633 NaN    VC71R NaN  ... NaN  00:00:00.010  00:00:23.260 NaN   \n",
      "4   00:00:01.920 NaN    VC72R NaN  ... NaN  00:00:00.010  00:00:23.260 NaN   \n",
      "5   00:00:00.450 NaN   AU101L NaN  ... NaN  00:00:00.010  00:00:23.260 NaN   \n",
      "6   00:00:01.950 NaN    AD38L NaN  ... NaN  00:00:00.030  00:00:00.670 NaN   \n",
      "7   00:00:02.054 NaN   AU145L NaN  ... NaN  00:00:00.330  00:00:01.120 NaN   \n",
      "8   00:00:02.300 NaN     AD51 NaN  ... NaN  00:00:00.410  00:00:00.950 NaN   \n",
      "9   00:00:03.000 NaN     AD1L NaN  ... NaN  00:00:00.740  00:00:01.110 NaN   \n",
      "10  00:00:03.060 NaN     AD19 NaN  ... NaN  00:00:01.030  00:00:01.140 NaN   \n",
      "11  00:00:04.280 NaN     AU10 NaN  ... NaN  00:00:01.040  00:00:02.350 NaN   \n",
      "12  00:00:05.820 NaN     AD81 NaN  ... NaN  00:00:01.080  00:00:01.460 NaN   \n",
      "13  00:00:16.660 NaN  EAD101R NaN  ... NaN  00:00:01.130  00:00:02.170 NaN   \n",
      "14  00:00:08.240 NaN     AU18 NaN  ... NaN  00:00:01.140  00:00:02.870 NaN   \n",
      "\n",
      "          77  78            79  80            81            82  \n",
      "0   FILM 12  NaN           NaN NaN           NaN           NaN  \n",
      "1      Code  NaN  Duration (s) NaN   Start time      End time   \n",
      "2       VC72 NaN         12.11 NaN  00:00:00.040  00:00:12.150  \n",
      "3      VC71R NaN         12.11 NaN  00:00:00.040  00:00:12.150  \n",
      "4      VC70R NaN         12.11 NaN  00:00:00.040  00:00:12.150  \n",
      "5     AU101L NaN          2.08 NaN  00:00:00.070  00:00:02.150  \n",
      "6       AD57 NaN          0.28 NaN  00:00:00.090  00:00:00.370  \n",
      "7       AD53 NaN          0.47 NaN  00:00:00.380  00:00:00.850  \n",
      "8    EAD104R NaN          0.33 NaN  00:00:01.030  00:00:01.360  \n",
      "9       AU5L NaN          0.85 NaN  00:00:02.160  00:00:03.010  \n",
      "10      AD1L NaN          0.85 NaN  00:00:02.160  00:00:03.010  \n",
      "11   EAD101R NaN          0.44 NaN  00:00:02.230  00:00:02.670  \n",
      "12   EAD104R NaN          0.55 NaN  00:00:03.010  00:00:03.560  \n",
      "13    AU101L NaN          9.28 NaN  00:00:03.230  00:00:12.480  \n",
      "14     AD38L NaN          1.96 NaN  00:00:03.910  00:00:05.870  \n",
      "\n",
      "[15 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "s2_raw = pd.read_excel(S2_PATH, sheet_name='FACS', header=None)\n",
    "print(\"First 15 rows of raw S3 annotation:\")\n",
    "print(s2_raw.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b61d145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 rows of raw S3 annotation:\n",
      "           0                            1                2                3  \\\n",
      "0   Media 1   Annotation date: 2019-03-22              NaN              NaN   \n",
      "1        NaN                          NaN              NaN              NaN   \n",
      "2        NaN                          NaN              NaN              NaN   \n",
      "3      Tier                           NaN      Begin time         End time    \n",
      "4       Ears                          NaN  00:00:00.640000  00:00:01.580000   \n",
      "5       Ears                          NaN  00:00:01.730000  00:00:02.300000   \n",
      "6       Ears                          NaN  00:00:04.480000  00:00:05.100000   \n",
      "7       Ears                          NaN  00:00:06.170000  00:00:06.740000   \n",
      "8       Ears                          NaN  00:00:06.780000  00:00:07.320000   \n",
      "9       Ears                          NaN  00:00:07.700000  00:00:07.960000   \n",
      "10      Ears                          NaN  00:00:07.960000  00:00:08.220000   \n",
      "11      Ears                          NaN  00:00:08.680000  00:00:08.990000   \n",
      "12      Ears                          NaN  00:00:10.090000  00:00:10.540000   \n",
      "13      Ears                          NaN  00:00:12.310000  00:00:12.790000   \n",
      "14      Ears                          NaN  00:00:12.800000  00:00:13.390000   \n",
      "\n",
      "                    4        5  \n",
      "0   Annotator: Alina       NaN  \n",
      "1                 NaN      NaN  \n",
      "2                 NaN      NaN  \n",
      "3           Duration      Code  \n",
      "4     00:00:00.940000   EAD101  \n",
      "5     00:00:00.570000  EAD104L  \n",
      "6     00:00:00.620000  EAD101L  \n",
      "7     00:00:00.570000  EAD104R  \n",
      "8     00:00:00.540000  EAD101R  \n",
      "9     00:00:00.260000  EAD104R  \n",
      "10    00:00:00.260000  EAD101R  \n",
      "11    00:00:00.310000  EAD104R  \n",
      "12    00:00:00.450000  EAD101R  \n",
      "13    00:00:00.480000  EAD104R  \n",
      "14    00:00:00.590000  EAD101R  \n"
     ]
    }
   ],
   "source": [
    "s3_raw = pd.read_excel(S3_PATH, sheet_name='FACS', header=None)\n",
    "print(\"First 15 rows of raw S3 annotation:\")\n",
    "print(s3_raw.head(15))\n",
    "# View any actual Media rows, AU codes, columns, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a598ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic: Main 1-12 as S1-S12; extras cycle (e.g., Film 17 → S5, Film 22 → S10, etc.)\n",
    "base_videos = ['S1_Video.mp4', 'S2_Video.mp4', 'S3_Video.mp4', 'S4_Video.mp4', 'S5_Video.mp4', 'S6_Video.mp4', \n",
    "               'S7_Video.mp4', 'S8_Video.mp4', 'S9_Video.mp4', 'S10_Video.mp4', 'S11_Video.mp4', 'S12_Video.mp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81b09191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDIA_TO_VIDEO dictionary updated with all skipped Media/Film mappings (using S1-S12 naming, cycled through 12 videos).\n"
     ]
    }
   ],
   "source": [
    "MEDIA_TO_VIDEO = {\n",
    "    # Your original main Media 1-12\n",
    "    'Media 1': base_videos[0], 'Media 2': base_videos[1], 'Media 3': base_videos[2],\n",
    "    'Media 4': base_videos[3], 'Media 5': base_videos[4], 'Media 6': base_videos[5],\n",
    "    'Media 7': base_videos[6], 'Media 8': base_videos[7], 'Media 9': base_videos[8],\n",
    "    'Media 10': base_videos[9], 'Media 11': base_videos[10], 'Media 12': base_videos[11],\n",
    "\n",
    "    # Skipped/Extra from log (cycled based on Film N → base_videos[(N-1) % 12])\n",
    "    'Media 2 (Film 17)': base_videos[(17-1) % 12],  # S5_Video.mp4\n",
    "    'Media 3 (Film 22)': base_videos[(22-1) % 12],  # S10_Video.mp4\n",
    "    'Media 4 (Film 23)': base_videos[(23-1) % 12],  # S11_Video.mp4\n",
    "    'Media 5 (Film 25)': base_videos[(25-1) % 12],  # S1_Video.mp4\n",
    "    'Media 6 (Film 27)': base_videos[(27-1) % 12],  # S3_Video.mp4\n",
    "    'Media 8 (Film 13)': base_videos[(13-1) % 12],  # S1_Video.mp4\n",
    "    'Media 9 (Film 14)': base_videos[(14-1) % 12],  # S2_Video.mp4\n",
    "    'Media 12 (Film 19)': base_videos[(19-1) % 12],  # S7_Video.mp4\n",
    "    'Media 13 (Film 28)': base_videos[(28-1) % 12],  # S4_Video.mp4\n",
    "    'Media 14 (Film 26)': base_videos[(26-1) % 12],  # S2_Video.mp4\n",
    "    'Media 15': base_videos[2],  # No Film; S3_Video.mp4 (sequential after 12)\n",
    "    'Media 17 (Film 16)': base_videos[(16-1) % 12],  # S4_Video.mp4\n",
    "    'Media 18 (Film 15)': base_videos[(15-1) % 12],  # S3_Video.mp4\n",
    "    'Media 19 (Film 21)': base_videos[(21-1) % 12],  # S9_Video.mp4\n",
    "    'Media 20 (Film 18)': base_videos[(18-1) % 12],  # S6_Video.mp4\n",
    "    'Media 21': base_videos[8],  # No Film; S9_Video.mp4\n",
    "    'Media 23 (Film 24)': base_videos[(24-1) % 12],  # S12_Video.mp4\n",
    "    'Media 25': base_videos[0],  # No Film; S1_Video.mp4\n",
    "    'Media 26 (Film 20)': base_videos[(20-1) % 12],  # S8_Video.mp4\n",
    "}\n",
    "\n",
    "print(\"MEDIA_TO_VIDEO dictionary updated with all skipped Media/Film mappings (using S1-S12 naming, cycled through 12 videos).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e45bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "EYES_AUS = [\n",
    "    'AD1', 'AD133', 'AD160', 'AD1L',\n",
    "    'AU101', 'AU101L', 'AU145', 'AU145 L',\n",
    "    'AU47', 'AU47L', 'AU5', 'AU5L'\n",
    "    ]\n",
    "CHIN_AUS = [\n",
    "    'AD38', 'AU10', 'AU113', 'AU17', 'AU18', 'AU24'\n",
    "    ]\n",
    "EYES_PATTERN = '|'.join(EYES_AUS)\n",
    "CHIN_PATTERN = '|'.join(CHIN_AUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125511a7",
   "metadata": {},
   "source": [
    "#### ----------- FUNCTIONS -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42cba62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clip(video_path, start_sec, end_sec, output_path, min_dur=0.5, max_dur=3.0):\n",
    "    \"\"\"Extracts segment from video and saves as output_path (mp4).\"\"\"\n",
    "    dur = end_sec - start_sec\n",
    "    if dur < min_dur:\n",
    "        padding = (min_dur - dur)/2\n",
    "        start_sec = max(0, start_sec - padding)\n",
    "        end_sec = start_sec + min_dur\n",
    "    elif dur > max_dur:\n",
    "        start_sec += (dur - max_dur)/2\n",
    "        end_sec = start_sec + max_dur\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not fps or fps < 1:\n",
    "        print(f\"[WARNING] No FPS detected for: {video_path}\")\n",
    "        return False\n",
    "\n",
    "    start_frame = int(start_sec * fps)\n",
    "    end_frame = int(end_sec * fps)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    for _ in range(end_frame - start_frame):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            out.write(frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11609bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_s3_media_segments(s3_path, eyes_pat, chin_pat):\n",
    "    raw = pd.read_excel(s3_path, sheet_name='FACS', header=None)\n",
    "    media_dfs = {}\n",
    "    current_media = None\n",
    "    start_row = 0\n",
    "    for idx, row in raw.iterrows():\n",
    "        cell0 = str(row[0]).strip() if pd.notna(row[0]) else ''\n",
    "        if cell0.startswith(\"Media\"):\n",
    "            if current_media is not None:\n",
    "                header_row = start_row + 3\n",
    "                nrows = idx - header_row\n",
    "                media_df = pd.read_excel(\n",
    "                    s3_path, sheet_name='FACS',\n",
    "                    skiprows=header_row, nrows=nrows\n",
    "                )\n",
    "                media_dfs[current_media] = media_df\n",
    "            current_media = cell0\n",
    "            start_row = idx\n",
    "    if current_media is not None:\n",
    "        header_row = start_row + 3\n",
    "        media_df = pd.read_excel(s3_path, sheet_name='FACS', skiprows=header_row)\n",
    "        media_dfs[current_media] = media_df\n",
    "\n",
    "    def pick_col(cols, keyword):\n",
    "        for c in cols:\n",
    "            if keyword in str(c).lower():\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    def to_sec(t):\n",
    "        if isinstance(t, str) and \":\" in t:\n",
    "            parts = [float(x) for x in t.split(\":\")]\n",
    "            return parts[0]*3600 + parts[1]*60 + parts[2]\n",
    "        try:\n",
    "            return float(t)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    eyes_segments, chin_segments = {}, {}\n",
    "    for media, df in media_dfs.items():\n",
    "        code_col = pick_col(df.columns, 'code')\n",
    "        begin_col = pick_col(df.columns, 'begin')\n",
    "        end_col = pick_col(df.columns, 'end')\n",
    "        if code_col and begin_col and end_col:\n",
    "            eyes_df = df[df[code_col].astype(str).str.contains(eyes_pat, na=False)]\n",
    "            chin_df = df[df[code_col].astype(str).str.contains(chin_pat, na=False)]\n",
    "            eyes_begin = eyes_df[begin_col].apply(to_sec)\n",
    "            eyes_end = eyes_df[end_col].apply(to_sec)\n",
    "            eyes_segments[media] = list(zip(eyes_begin, eyes_end))\n",
    "            chin_begin = chin_df[begin_col].apply(to_sec)\n",
    "            chin_end = chin_df[end_col].apply(to_sec)\n",
    "            chin_segments[media] = list(zip(chin_begin, chin_end))\n",
    "    return eyes_segments, chin_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6231c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_s2_film_segments(s2_path, eyes_pat, chin_pat):\n",
    "    s2_df = pd.read_excel(s2_path, sheet_name='FACS')\n",
    "    film_cols = [col for col in s2_df.columns if 'code' in str(col).lower()]\n",
    "    eyes_segments, chin_segments = {}, {}\n",
    "    for col in film_cols:\n",
    "        film_name = next((f for f in s2_df.columns if f in str(col)), None)\n",
    "        eyes_rows = s2_df[s2_df[col].astype(str).str.contains(eyes_pat, na=False)]\n",
    "        chin_rows = s2_df[s2_df[col].astype(str).str.contains(chin_pat, na=False)]\n",
    "        begin_col = pick_col(s2_df.columns, 'begin')\n",
    "        end_col = pick_col(s2_df.columns, 'end')\n",
    "        def to_sec(t):\n",
    "            if isinstance(t, str) and \":\" in t:\n",
    "                parts = [float(x) for x in t.split(\":\")]\n",
    "                return parts[0]*3600 + parts[1]*60 + parts[2]\n",
    "            try:\n",
    "                return float(t)\n",
    "            except Exception:\n",
    "                return 0.0\n",
    "        if not (begin_col and end_col):\n",
    "            continue\n",
    "        eyes_begin = eyes_rows[begin_col].apply(to_sec)\n",
    "        eyes_end = eyes_rows[end_col].apply(to_sec)\n",
    "        chin_begin = chin_rows[begin_col].apply(to_sec)\n",
    "        chin_end = chin_rows[end_col].apply(to_sec)\n",
    "        eyes_segments[film_name] = list(zip(eyes_begin, eyes_end))\n",
    "        chin_segments[film_name] = list(zip(chin_begin, chin_end))\n",
    "    return eyes_segments, chin_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02758c4d",
   "metadata": {},
   "source": [
    "#### ----------- MAIN LOGIC -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a99220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Eyes segments keys: ['Media 1', 'Media 2 (Film 17)', 'Media 3 (Film 22)', 'Media 4 (Film 23)', 'Media 5 (Film 25)', 'Media 6 (Film 27)', 'Media 7', 'Media 8 (Film 13)', 'Media 9 (Film 14)', 'Media 12 (Film 19)', 'Media 13 (Film 28)', 'Media 14 (Film 26)', 'Media 15', 'Media 17 (Film 16)', 'Media 18 (Film 15)', 'Media 19 (Film 21)', 'Media 20 (Film 18)', 'Media 21', 'Media 23 (Film 24)', 'Media 25', 'Media 26 (Film 20)']\n",
      "S3 Chin segments keys: ['Media 1', 'Media 2 (Film 17)', 'Media 3 (Film 22)', 'Media 4 (Film 23)', 'Media 5 (Film 25)', 'Media 6 (Film 27)', 'Media 7', 'Media 8 (Film 13)', 'Media 9 (Film 14)', 'Media 12 (Film 19)', 'Media 13 (Film 28)', 'Media 14 (Film 26)', 'Media 15', 'Media 17 (Film 16)', 'Media 18 (Film 15)', 'Media 19 (Film 21)', 'Media 20 (Film 18)', 'Media 21', 'Media 23 (Film 24)', 'Media 25', 'Media 26 (Film 20)']\n"
     ]
    }
   ],
   "source": [
    "# ----------- SEGMENT PARSING -----------\n",
    "\n",
    "# --- Parse segments independently --\n",
    "eyes_segments_s3, chin_segments_s3 = parse_s3_media_segments(S3_PATH, EYES_PATTERN, CHIN_PATTERN)\n",
    "eyes_segments_s2, chin_segments_s2 = parse_s2_film_segments(S2_PATH, EYES_PATTERN, CHIN_PATTERN)\n",
    "print(\"S3 Eyes segments keys:\", list(eyes_segments_s3.keys()))\n",
    "print(\"S3 Chin segments keys:\", list(chin_segments_s3.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1e8e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge results for extraction ---\n",
    "eyes_segments = eyes_segments_s3.copy()\n",
    "chin_segments = chin_segments_s3.copy()\n",
    "for film in eyes_segments_s2:\n",
    "    media_key = film.replace('FILM ', 'Media ')\n",
    "    if media_key not in eyes_segments:\n",
    "        eyes_segments[media_key] = eyes_segments_s2[film]\n",
    "        chin_segments[media_key] = chin_segments_s2[film]\n",
    "    else:\n",
    "        eyes_segments[media_key].extend(eyes_segments_s2[film])\n",
    "        chin_segments[media_key].extend(chin_segments_s2[film])\n",
    "\n",
    "clip_metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bfba2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Loop ---\n",
    "for media in eyes_segments:\n",
    "    video_file = MEDIA_TO_VIDEO.get(media)\n",
    "    if not video_file:\n",
    "        print(f\"[SKIP] No video mapping for: {media}\")\n",
    "        continue\n",
    "    video_path = os.path.join(VIDEOS_DIR, video_file)\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"[SKIP] Video not found: {video_path}\")\n",
    "        continue\n",
    "    # --- Eyes Positive ---\n",
    "    for i, (start, end) in enumerate(eyes_segments.get(media, [])):\n",
    "        out_path = os.path.join(CLIPS_DIR, 'eyes_positive', f\"{media}_clip{i}.mp4\")\n",
    "        ok = extract_clip(video_path, start, end, out_path)\n",
    "        if ok:\n",
    "            clip_metadata.append({'clip': out_path, 'eyes_present': 1, 'chin_present': 0, 'nostrils_present': None})\n",
    "    # --- Chin Positive ---\n",
    "    for i, (start, end) in enumerate(chin_segments.get(media, [])):\n",
    "        out_path = os.path.join(CLIPS_DIR, 'chin_positive', f\"{media}_clip{i}.mp4\")\n",
    "        ok = extract_clip(video_path, start, end, out_path)\n",
    "        if ok:\n",
    "            clip_metadata.append({'clip': out_path, 'eyes_present': 0, 'chin_present': 1, 'nostrils_present': None})\n",
    "    # --- Negative Clips ---\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_dur = cap.get(cv2.CAP_PROP_FRAME_COUNT) / (cap.get(cv2.CAP_PROP_FPS) or 1)\n",
    "    cap.release()\n",
    "    all_pos = sorted(set(eyes_segments.get(media, []) + chin_segments.get(media, [])))\n",
    "    gaps = [(0, all_pos[0][0])] if all_pos else [(0, total_dur)]\n",
    "    for j in range(len(all_pos)-1):\n",
    "        gaps.append((all_pos[j][1], all_pos[j+1][0]))\n",
    "    if all_pos:\n",
    "        gaps.append((all_pos[-1][1], total_dur))\n",
    "    gaps = [g for g in gaps if g[1] - g[0] >= 0.5]\n",
    "    num_neg = min(145, len(gaps))\n",
    "    for i in range(num_neg):\n",
    "        gap = random.choice(gaps)\n",
    "        max_len = min(3.0, gap[1] - gap[0])\n",
    "        if max_len < 0.5:\n",
    "            continue\n",
    "        start = random.uniform(gap[0], gap[1] - max_len)\n",
    "        end = start + random.uniform(0.5, max_len)\n",
    "        out_path = os.path.join(CLIPS_DIR, 'negative', f\"{media}_neg_clip{i}.mp4\")\n",
    "        ok = extract_clip(video_path, start, end, out_path)\n",
    "        if ok:\n",
    "            clip_metadata.append({'clip': out_path, 'eyes_present': 0, 'chin_present': 0, 'nostrils_present': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f89906cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 complete: 1211 total clips (847 train, 182 val, 182 test).\n"
     ]
    }
   ],
   "source": [
    "# --- SPLIT & SAVE ---\n",
    "CSV_DIR = os.path.join(project_root, 'outputs', 'csv')\n",
    "clip_df = pd.DataFrame(clip_metadata)\n",
    "if not clip_df.empty:\n",
    "    labels = clip_df['eyes_present'] + 2 * clip_df['chin_present']\n",
    "    train, temp = train_test_split(clip_df, test_size=0.3, stratify=labels, random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, stratify=temp['eyes_present'] + 2 * temp['chin_present'], random_state=42)\n",
    "\n",
    "    train.to_json(os.path.join(CSV_DIR, 'train_metadata.json'), orient='records')\n",
    "    val.to_json(os.path.join(CSV_DIR, 'val_metadata.json'), orient='records')\n",
    "    test.to_json(os.path.join(CSV_DIR, 'test_metadata.json'), orient='records')\n",
    "    train.to_csv(os.path.join(CSV_DIR, 'train_metadata.csv'), index=False)\n",
    "    val.to_csv(os.path.join(CSV_DIR, 'val_metadata.csv'), index=False)\n",
    "    test.to_csv(os.path.join(CSV_DIR, 'test_metadata.csv'), index=False)\n",
    "\n",
    "    print(f\"Task 1 complete: {len(clip_df)} total clips ({len(train)} train, {len(val)} val, {len(test)} test).\")\n",
    "else:\n",
    "    print(\"No clips extracted—check segment extraction and video mapping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
